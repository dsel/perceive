{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "We are interested in observing the discussions on Twitter to identify vulnerabilites and exposures.\n",
    "We plan to focus on collecting tweets consisting of particular words of interest. The data collected can then be cleaned and used for analysis on similar lines as the full disclosure mailing list.\n",
    "\n",
    "The notebook is inspired by the work in the article [Vulnerability Disclosure in the Age of Social Media: Exploiting Twitter for Predicting Real-World Exploits](https://www.umiacs.umd.edu/~tdumitra/papers/USENIX-SECURITY-2015.pdf) where the idea is to use twitter analytics for early detecting exploits. There have been instances shared in the [presentation](http://www.umiacs.umd.edu/~tdumitra/blog/2015/08/02/predicting-vulnerability-exploits/), where vulnerabilities have been mentioned and discussed on twitter before the vulnerability is disclosed and this is the motivation for using twitter as a part of the research.\n",
    "\n",
    "The researchers in the article started collecting tweets based on a list of 50 words. The list of words have not been mentioned in the article, hence we start our analysis by collecting tweets for keywords identified manually by doing our research on discussions related to vulnerabilities on twitter.\n",
    "\n",
    "The following task can be achieved in two ways:\n",
    "1. Search for historical tweets with specific words of interest using the **search API**\n",
    "2. Monitor the feed on twitter for specific words of interest using the **streaming API**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myvars = {}\n",
    "with open(\"Twitter_keys.txt\") as myfile:\n",
    "    for line in myfile:\n",
    "        name, var = line.partition(\"=\")[::2]\n",
    "        myvars[name.strip()] = var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "APP_KEY = myvars[\"APP_KEY\"].rstrip()\n",
    "APP_SECRET = myvars[\"APP_SECRET\"].rstrip()\n",
    "OAUTH_TOKEN = myvars[\"OAUTH_TOKEN\"].rstrip()\n",
    "OAUTH_TOKEN_SECRET = myvars[\"OAUTH_TOKEN_SECRET\"].rstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tweet Extraction\n",
    "\n",
    "We use the [twython](https://github.com/ryanmcgrath/twython) library for the tweet extraction. Twython is an actively maintained, pure Python wrapper for the Twitter API. It supports both normal and streaming Twitter APIs.\n",
    "\n",
    "The primary task is to obtain the keys and tokens required to access the API and then access the functions in the wrappers.\n",
    "\n",
    "The scripts below have two configurable parameters:\n",
    "1. The **query_word** variable needs to be initialized with the keywords we are looking for on the twitter feed\n",
    "2. The **max_tweets** variable is the number of tweets we plan to extract for the keywords mentioned above\n",
    "    \n",
    "If query_word is initialized to multiple words, the code will retrieve set of tweets that consists of all the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Streaming API\n",
    "\n",
    "The streaming API is used to access all the current tweets. It returns approximately 1% of the tweets i.e. 60 tweets per second assuming a maximum of 6000 users tweet every second. There is no rate limit to the Streaming API.\n",
    "\n",
    "The hyperlink has details related to the Twitter Streaming API limit:\n",
    "1. [URL 1](https://stackoverflow.com/questions/34962677/twitter-streaming-api-limits)\n",
    "2. [URL 2](https://stackoverflow.com/questions/13055370/how-many-percent-of-the-tweets-does-twitter-sample-api-give)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max tweets extracted\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "from twython import TwythonStreamer\n",
    "from twython import Twython, TwythonError\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "#Configurable parameters\n",
    "query_word=\"flaw\"\n",
    "max_tweets=2\n",
    "\n",
    "\n",
    "searched_tweets_strm=[]\n",
    "\n",
    "class MyStreamer(TwythonStreamer):\n",
    "    def on_success(self, data):\n",
    "        if len(searched_tweets_strm)<max_tweets:\n",
    "            if 'text' in data:\n",
    "                #print data['text'].encode('utf-8')\n",
    "                searched_tweets_strm.append(data['text'].encode('utf-8'))\n",
    "            else:\n",
    "                print (\"No tweets found\")\n",
    "                self.disconnect()\n",
    "        else: \n",
    "            print (\"Max tweets extracted\")\n",
    "            sys.exit()\n",
    "\n",
    "    def on_error(self, status_code, data):\n",
    "        print (status_code, data)\n",
    "        print (\"Exception raised, waiting 15 minutes\")\n",
    "        time.sleep(15*60)\n",
    "\n",
    "# Requires Authentication as of Twitter API v1.1\n",
    "stream = MyStreamer(APP_KEY, APP_SECRET,\n",
    "                    OAUTH_TOKEN, OAUTH_TOKEN_SECRET)\n",
    "\n",
    "\n",
    "stream.statuses.filter(track=query_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (searched_tweets_strm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Search API\n",
    "\n",
    "The streaming API is used to collect tweets from the current feed and not the historical data. For historical data, the search API can be used to access tweets that are up to [three weeks old](https://stackoverflow.com/questions/1662151/getting-historical-data-from-twitter).\n",
    "Once these historical tweets are collected, the rest of the tweets can be accessed by running the streaming API continuously.\n",
    "\n",
    "**Limitations:**\n",
    "To collect tweets for two different words, the Twitter API needs to be queried twice and there is no functionality of collecting it in one function call at the same time.\n",
    "\n",
    "**Example:**\n",
    "If we need to collect tweets with the word \"data\" in it and we also need to collect tweets with the word with the word \"flaw\" in it. The search API needs to be called twice, once for the word \"data\" and then for the word \"flaw\".\n",
    "1. twitter.search(q=\"data\")\n",
    "2. twitter.search(q=\"flaw\")\n",
    "\n",
    "Note:\n",
    "The search API limit is 180 requests every 15 minutes and hence the code will sleep for 15 minutes every time the API limit is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets extracted for flaw: 200\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "from twython import Twython, TwythonError\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "\n",
    "#Configurable parameters\n",
    "query_word=\"flaw\"\n",
    "max_tweets=200\n",
    "\n",
    "\n",
    "tweet_cnt=0\n",
    "# Requires Authentication as of Twitter API v1.1\n",
    "twitter = Twython(APP_KEY, APP_SECRET, OAUTH_TOKEN, OAUTH_TOKEN_SECRET)\n",
    "\n",
    "searched_tweets_srch = []\n",
    "while len(searched_tweets_srch) < max_tweets:\n",
    "    remaining_tweets = max_tweets - len(searched_tweets_srch)\n",
    "    try:\n",
    "        search_results = twitter.search(q=query_word, count=100)\n",
    "        \n",
    "        if not search_results:\n",
    "            print('no tweets found')\n",
    "            break\n",
    "            \n",
    "        tweet_cnt=tweet_cnt+len(search_results[\"statuses\"])\n",
    "        searched_tweets_srch.extend(search_results[\"statuses\"])\n",
    "        \n",
    "    except TwythonError as e:\n",
    "        print (e)\n",
    "        print (\"exception raised, waiting 16 minutes\")\n",
    "        print (strftime(\"%H:%M:%S\"+ gmtime()))\n",
    "        time.sleep(16*60)\n",
    "\n",
    "print (\"Total tweets extracted for \"+query_word+\": \"+str(tweet_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (searched_tweets_srch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "1. Vulnerability Disclosure in the Age of Social Media: Exploiting Twitter for Predicting Real-World Exploits,\n",
    "https://www.umiacs.umd.edu/~tdumitra/papers/USENIX-SECURITY-2015.pdf\n",
    "\n",
    "2. http://www.umiacs.umd.edu/~tdumitra/blog/2015/08/02/predicting-vulnerability-exploits/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
